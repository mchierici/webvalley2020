{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "TJfNUgelLKKI"
   },
   "source": [
    "# <center>Machine learning from scratch - Part II</center>\n",
    "## <center>WebValley reimagined 2020</center>\n",
    "### <center>Marco Chierici</center>\n",
    "#### <center>FBK/MPBA</center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "fuVtNnbq1o3z"
   },
   "source": [
    "In this handout we will go through basic concepts of machine learning using Python and scikit-learn on a real-world dataset of biological relevance [Zhang et al, _Genome Biology_ , 2015]."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Neuroblastoma dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "n012d52Z1x2_"
   },
   "source": [
    "In this section, we will focus on the SEQC Neuroblastoma dataset, specifically on **a subset of 272 samples (136 training, 136 test)**, aiming at predicting an **extreme disease outcome** (favorable vs unfavorable samples: see [main paper](https://www.ncbi.nlm.nih.gov/pubmed/26109056))."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"images/zhang.png\" width=\"65%\" />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"images/zhang_tab2.png\" width=\"95%\" />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "nhboH88jk3Wa"
   },
   "source": [
    "The data was preprocessed a bit to facilitate the progress of the tutorial."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "NXauQAQK16-V"
   },
   "source": [
    "Let's start by loading a few modules that we'll be using later:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "ww5yKJJpk3Wc"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pylab as plt ## for plotting\n",
    "import pandas as pd ## for reading text files and manipulating data frames\n",
    "from sklearn import neighbors ## kNN classifier\n",
    "from pathlib import Path ## for creating paths in a neat way\n",
    "np.random.seed(42) ## set random seed just in case"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "X1JAqMhok3Wf"
   },
   "source": [
    "Define files to read:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "VjZyyr-EoCQw"
   },
   "outputs": [],
   "source": [
    "##  for convenience, define the data directory as a variable\n",
    "DATA_DIR = Path(\"data\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "z35eGqzck3Wg"
   },
   "outputs": [],
   "source": [
    "DATA_TR = DATA_DIR / \"MAV-G_272_tr.txt\"\n",
    "DATA_TS = DATA_DIR / \"MAV-G_272_ts.txt\"\n",
    "LABS_TR = DATA_DIR / \"labels_tr.txt\"\n",
    "LABS_TS = DATA_DIR / \"labels_ts.txt\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "LABS_TS"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "_Note:_ from now on we will use the \"tr\" suffix to denote the training set, and \"ts\" for the test set."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "QywnF5Rek3Wj"
   },
   "source": [
    "Read the files in as _pandas dataframes_ (they are conceptually like R data frames):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 864
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 1345,
     "status": "error",
     "timestamp": 1554388310330,
     "user": {
      "displayName": "Marco Chierici",
      "photoUrl": "https://lh6.googleusercontent.com/-1LjDBMGAnW8/AAAAAAAAAAI/AAAAAAAACB0/ScmrJqjZC-4/s64/photo.jpg",
      "userId": "06871654247545486268"
     },
     "user_tz": -120
    },
    "id": "cYnTRFVyk3Wk",
    "outputId": "a91effd6-a06f-42b9-8911-0ee006aaa08a"
   },
   "outputs": [],
   "source": [
    "data_tr = pd.read_csv(DATA_TR, sep=\"\\t\")\n",
    "data_ts = pd.read_csv(DATA_TS, sep=\"\\t\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "THEk0UfNqZqZ"
   },
   "source": [
    "The function `read_csv` has a lot more input arguments to deal with different situations.\n",
    "\n",
    "If you want to know more about this or any other Python function, use the `help(function_name)` command or `function_name?`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "UnTyIsHRk3Wm"
   },
   "source": [
    "Give a look at what we have here, start with getting the dimensions of what we just uploaded:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "2iltS1Q-k3Wn",
    "outputId": "ea81462d-8a49-406b-c933-182c49379053"
   },
   "outputs": [],
   "source": [
    "data_tr.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "_kRnmCcI2Wo4"
   },
   "source": [
    "What's inside?\n",
    "\n",
    "A peek at the first rows reveals that the first column (the dataframe index) contains the sample IDs, and the remaining columns are genes:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "TVfPwU6-k3Wt",
    "outputId": "884dd460-6c53-4bf4-9c37-f7c7299b37a2"
   },
   "outputs": [],
   "source": [
    "data_tr.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "-Q-p18V3k3Ww"
   },
   "source": [
    "Drop the first column from the train and test expression sets, since it's just the sample IDs (we put them in to be able to check whether samples and labels match, but once we are sure of what we are doing we don't really need them anymore)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "F1bUOeE4k3Wx"
   },
   "outputs": [],
   "source": [
    "data_tr = data_tr.drop('sampleID', axis=1)\n",
    "data_ts = data_ts.drop('sampleID', axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "WA_GLQAWk3W0"
   },
   "source": [
    "Check what happened"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "QgcQgBVek3W1",
    "outputId": "6cbec2e0-0001-4e03-c040-0bbddd51db5b"
   },
   "outputs": [],
   "source": [
    "data_tr.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "NTZp-z-jk3W4"
   },
   "source": [
    "Read in the files containing label information and check how they look like."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "7Vbq9mqXk3W5",
    "outputId": "6bc4f986-30e2-4953-fc78-80b80982ba47"
   },
   "outputs": [],
   "source": [
    "labs_tr = pd.read_csv(LABS_TR, sep = \"\\t\")\n",
    "labs_ts = pd.read_csv(LABS_TS, sep = \"\\t\")\n",
    "labs_tr.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Bw2huNKIk3W8"
   },
   "source": [
    "We have to fit one model for each label type, so we need to select one column at a time. We start with CLASS, in principle we could consider looping over the columns of interest. In this case no need to remove the first column, since we are using one column at a time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "pyTfzujJk3W9",
    "outputId": "cd7cf62a-c5b1-491a-853e-631b5cc9a4d2"
   },
   "outputs": [],
   "source": [
    "class_lab_tr = labs_tr[['CLASS']]\n",
    "class_lab_ts = labs_ts[['CLASS']]\n",
    "## give a look at one of the two\n",
    "class_lab_tr.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "7HSRblhv3RJY"
   },
   "source": [
    "For the remaining part of this hands-on, we need the data and labels to be stored in Numpy arrays (remember the `.values` function?):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "6RAvCkTE3bns"
   },
   "outputs": [],
   "source": [
    "x_tr = data_tr.values\n",
    "x_ts = data_ts.values\n",
    "\n",
    "y_tr = class_lab_tr.values.ravel()\n",
    "y_ts = class_lab_ts.values.ravel()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_tr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_tr.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "PoeFheJ9p9wG"
   },
   "source": [
    "\n",
    "\n",
    "---\n",
    "\n",
    "\n",
    "*Naming conventions: in the machine learning world, usually `x` is the data and `y` the target variable (the labels). *\n",
    "\n",
    "---\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "kqmSqRd23lCS"
   },
   "source": [
    "When coding, it is a good practice to have a peek at the resulting variables, to be sure everything is OK: i.e., is that variable like it is supposed to be? Did I accidentally throw away a feature column?\n",
    "\n",
    "This can avoid lots of problems later on!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "XYs3b6JJ3qrn"
   },
   "outputs": [],
   "source": [
    "x_tr.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "-VrsQZKO37wt"
   },
   "source": [
    "Let's go back to the sample labels:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "iwI8uSvC4BbC"
   },
   "outputs": [],
   "source": [
    "y_tr.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "f99gjxxW3yMs"
   },
   "source": [
    "---\n",
    "\n",
    "### Quick recap\n",
    "\n",
    "- class_lab_tr = 0 indicates **unfavorable** neuroblastoma samples (**bad** outcome)\n",
    "- class_lab_tr = 1 indicates **favorable** neuroblastoma samples (**good** outcome)\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The downstream analysis can benefit from data preprocessing, i.e., rescaling or standardizing data values."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In Scikit learn you can use `MinMaxScaler` or `StandardScaler` in the `preprocessing` submodule. Here is an example using the `MinMaxScaler`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import MinMaxScaler\n",
    "## first you need to create a \"scaler\" object\n",
    "scaler = MinMaxScaler(feature_range=(-1,1))\n",
    "## then you actually scale data by fitting the scaler object on the data\n",
    "scaler.fit(x_tr)\n",
    "x_tr = scaler.transform(x_tr)\n",
    "x_ts = scaler.transform(x_ts)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note how we transformed the test set: we computed the scaling parameters on the training set and applied them to the test set. In this way, we did not use any information in the test set to standardize it."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "gDUP1kW8ou4-"
   },
   "source": [
    "# Data exploration"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Remember the scatterplot matrix we showed on the 4-feature Iris data?\n",
    "\n",
    "_How should we visualize feature relationships on the neuroblastoma dataset with 50K features? Would it be a good idea to compute a scatterplot matrix?_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "yrQII8Zm4SQ-"
   },
   "source": [
    "It is probably better to first reduce the dimensionality of our data.\n",
    "\n",
    "Principal Component Analysis (PCA) is one example of data dimesionality reduction technique. It finds a sequence of linear combination of the variables (called the _principal components_) that explain the maximum variance and summarize the most information in the data and are mutually uncorrelated."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "LrgsMWEHoyzd"
   },
   "source": [
    "Let's perform an **unsupervised learning** task on our data set \"as is\" by decomposing it in its Principal Components.\n",
    "\n",
    "In scikit-learn, we can use the module PCA:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "pEQSdHYi5F4k"
   },
   "outputs": [],
   "source": [
    "from sklearn.decomposition import PCA\n",
    "pca = PCA(n_components=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "_P4v7UyA5I_Z"
   },
   "source": [
    "So far we have a PCA _object_ but no transformation yet.\n",
    "\n",
    "To actually transform the data, we'll have to _fit_ the PCA object on the training data, and then _transforming_ them in the Principal Component space:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pca.fit(x_tr)\n",
    "z_tr = pca.transform(x_tr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "z_tr.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_tr.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "GSURQjcg6ERt"
   },
   "source": [
    "Let's have a look at the _variance ratio_, i.e. the percentage of the variance explained by each component:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Cma7FaOd6F1M"
   },
   "outputs": [],
   "source": [
    "print(pca.explained_variance_ratio_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "gf_TACYP6JXk"
   },
   "source": [
    "_What can you understand from these variance percentages? Could this task be \"predictable\" by some sort of model?_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "4oM7HM9d6UL6"
   },
   "source": [
    "Is it always convenient to visualize the first two principal components in a scatterplot, in order to get a first assessment of the goodness of the decomposition.\n",
    "\n",
    "We will color the points in the plot according to our sample labels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "o8Ktj-fK6WWo"
   },
   "outputs": [],
   "source": [
    "f = plt.figure()\n",
    "plt.scatter(z_tr[y_tr == 0, 0], z_tr[y_tr == 0, 1], color=\"r\")\n",
    "plt.scatter(z_tr[y_tr == 1, 0], z_tr[y_tr == 1, 1], color=\"b\")\n",
    "plt.title(\"PCA of Train data\")\n",
    "plt.xlabel(\"PC1\")\n",
    "plt.ylabel(\"PC2\")\n",
    "plt.show()\n",
    "f.savefig(\"PCA_train.pdf\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "1GHx_kFZ6k5T"
   },
   "source": [
    "_Now apply the transformation to the test data, plot it, and save it as PDF._"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## exercise here\n",
    "\n",
    "# method 1: PCA on the test set\n",
    "pca.fit(x_ts)\n",
    "z_ts = pca.transform(x_ts)\n",
    "\n",
    "# method 2: PCA on the train, use it to transform the test set\n",
    "pca.fit(x_tr)\n",
    "z_ts = pca.transform(x_ts)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We now perform a UMAP transformation on the data, recalling what we did on the Iris dataset. Notice how we now split the `fit` and the `transform` operations:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import umap\n",
    "reducer = umap.UMAP(random_state=11)\n",
    "reducer.fit(x_tr)\n",
    "embedding = reducer.transform(x_tr)\n",
    "embedding.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_2D = pd.DataFrame(embedding,\n",
    "                     columns=['UMAP1', 'UMAP2'])\n",
    "df_2D['class'] = y_tr\n",
    "df_2D.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for key, group in df_2D.groupby(['class']):\n",
    "    plt.plot(group.UMAP1, group.UMAP2, 'o', alpha=0.7, label=key)\n",
    "    \n",
    "plt.legend(loc=0, fontsize=12)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once we have a UMAP object projecting the training set into a low-dimensional space, we can project the test set onto the same manifold. \n",
    "\n",
    "We use the `transform` method on the reducer object, and plot the resulting transformation:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding_ts = reducer.transform(x_ts)\n",
    "df_2D = pd.DataFrame(embedding_ts,\n",
    "                     columns=['UMAP1', 'UMAP2'])\n",
    "df_2D['class'] = y_ts\n",
    "\n",
    "for key, group in df_2D.groupby(['class']):\n",
    "    plt.plot(group.UMAP1, group.UMAP2, 'o', alpha=0.7, label=key)\n",
    "    \n",
    "plt.legend(loc=0, fontsize=12)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "FErPMN8F6sb9"
   },
   "source": [
    "## Supervised Learning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "T7xRxdjO6vTY"
   },
   "source": [
    "### k-NN classifier"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "LnHfIehl62Js"
   },
   "source": [
    "Based on the PCA we built on our data, we decide to try some supervised learning on them.\n",
    "\n",
    "Scikit-learn provides you access to several models via a very convenient _fit_ and _predict_ interface.\n",
    "\n",
    "For example, let's fit a **k-NN** model on the whole training data and then use it to predict the labels of the test data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "6P24VUwQk3XG"
   },
   "outputs": [],
   "source": [
    "knn = neighbors.KNeighborsClassifier(n_neighbors=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Cg8TpDATk3XI",
    "outputId": "e9658389-474c-4bf5-f196-11d4518311b7"
   },
   "outputs": [],
   "source": [
    "knn.fit(x_tr, y_tr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "B5pHIVW2k3XM"
   },
   "outputs": [],
   "source": [
    "y_pred_knn = knn.predict(x_ts) # predict labels on test data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Crr_2DwX7f1Z"
   },
   "source": [
    "_In general, a classifier has **parameters** that need to be tuned. Default choices are not good in all situations._\n",
    "\n",
    "_For example, in k-NN the main parameter is the **number of neighbors** used in the nearest neighbors algorithm._\n",
    "\n",
    "_More on this in the next lecture!_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "aBsDZNnXk3XP"
   },
   "source": [
    "To evaluate the predictions we need some kind of metrics. \n",
    "\n",
    "_Exercise: extract confusion matrix and try calculate metrics by hand._"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "L88ilJJG-Cnf"
   },
   "source": [
    "### Recap: confusion matrix\n",
    "\n",
    "In this example, the first row is class 0, so the confusion matrix will look like:\n",
    "\n",
    "|      |  |  Predicted  |    |\n",
    "|------|-----------|----|----|\n",
    "|      |           | 0 | 1  |\n",
    "| True | 0        | TN | FP |\n",
    "|      | 1         | FN | TP |\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "JISD2EVQ9Q9Z"
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "conf = confusion_matrix(y_ts, y_pred_knn)\n",
    "conf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Kh9MqHB5cC43"
   },
   "source": [
    "The total number of class 0 test samples (AN = All Negatives) should be equal to the sum of the first row of the confusion matrix, i.e., TN + FP:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "pZVN8GKKdOhy"
   },
   "outputs": [],
   "source": [
    "np.sum(y_ts==0) # total number of \"class 0\" samples in the test set"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "SoBqoaTGcDVy"
   },
   "source": [
    "Similarly for class 1, i.e., AP = All Positives = TP + FN:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "1PVj7JbxdVk0"
   },
   "outputs": [],
   "source": [
    "np.sum(y_ts==1) # total number of \"class 1\" samples in the test set"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "kva3wkz5ddap"
   },
   "source": [
    "Compute the Accuracy, remembering/using the formula: \n",
    "\n",
    "ACC = (TN + TP) / (TN + TP + FN + FP)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "q0emRGAvfWi4"
   },
   "outputs": [],
   "source": [
    "tp = conf[1,1]\n",
    "tn = conf[0,0]\n",
    "fp = conf[0,1]\n",
    "fn = conf[1,0]\n",
    "\n",
    "acc = (tn + tp) / (tn + tp + fn + fp)\n",
    "print(acc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "iqD0_lXneEJM"
   },
   "source": [
    "Now compute the Sensitivity.\n",
    "\n",
    "Remember the formula:\n",
    "\n",
    "SENS = TP / (TP + FN)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "a9JlR-LNe5ZI"
   },
   "outputs": [],
   "source": [
    "tp / (tp + fn)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "jcKt3dqwes-k"
   },
   "source": [
    "Computing metrics by hand is good, but what about a quicker option?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "opG9Fe6EkI_b"
   },
   "source": [
    "As seen in the lectures, Scikit Learn offers a handy broad range of functions to evaluate your classifier through its submodule `metrics`.\n",
    "\n",
    "Let's compute the accuracy using the scikit-learn built-in function `accuracy_score`, taking as input the predicted labels (`y_pred_knn`) and the true labels (`y_ts`):\n",
    "\n",
    "`accuracy_score(y_ts, y_pred_knn)`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "3KeLJcCbkSo6"
   },
   "outputs": [],
   "source": [
    "from sklearn import metrics\n",
    "metrics.accuracy_score(y_ts, y_pred_knn)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "r7owZk6BmTVX"
   },
   "source": [
    "What about Sensitivity? The built-in function is called `recall_score`, as Recall is an alternate name for Sensitivity. Again, its input are the predicted and the true labels.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "MgfhssjZmsg3"
   },
   "outputs": [],
   "source": [
    "metrics.recall_score(y_ts, y_pred_knn)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "8ysD26nCnTSg"
   },
   "source": [
    "Scikit-learn also provides a neat `metrics.classification_report` function that outputs a few metrics stratified by class:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "HXgvIJM2k3XQ",
    "outputId": "0d2d0773-a292-40cb-d8e7-df6b6ee29ff2"
   },
   "outputs": [],
   "source": [
    "print(metrics.classification_report(y_ts, y_pred_knn))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "mqAX0k6UoeZh"
   },
   "source": [
    "Let's consider the Matthews Correlation Coefficient (MCC):"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Dl82PZ0Zn5lN"
   },
   "source": [
    "![MCC formula](https://www.researchgate.net/profile/Pablo_Moscato/publication/223966631/figure/fig1/AS:305103086080001@1449753652505/Calculation-of-Matthews-Correlation-Coefficient-MCC-A-Contingency-matrix_W640.jpg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "4v0aWAAJrex-"
   },
   "source": [
    "*Q: Do you remember the main features of MCC?*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "FDN2f1ZAoty0"
   },
   "source": [
    "In scikit-learn it is computed by the `metrics.matthews_corrcoef` function."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "ynxyEsfFk3XV"
   },
   "source": [
    "If we get the MCC for our kNN predictions, we can observe that it is in line with our *a priori* knowledge of the dataset (from the article):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "OuoRfictk3XW",
    "outputId": "9119acba-9d18-4076-eb3c-8346ba420579"
   },
   "outputs": [],
   "source": [
    "print(metrics.matthews_corrcoef(y_ts, y_pred_knn))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "HtPbufFSpSxL"
   },
   "source": [
    "*Compare the metrics that you computed so far. What can you say about this classification task? Does the classifier learn something?*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "-d0e4tAbs7tP"
   },
   "source": [
    "The metrics may look good (e.g., accuracy around 0.8, MCC above 0.6) but...\n",
    "\n",
    "... how do you know if this model performs similarly well on unseen data?\n",
    "\n",
    "In other words, does this model *generalize* beyond its training set?\n",
    "\n",
    "This is why *data partitioning* techniques are used."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "13LEke6Js720"
   },
   "source": [
    "## Data partitioning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "-IIhs_0Exo1B"
   },
   "source": [
    "### Hold-out strategy\n",
    "\n",
    "The idea behind data partitioning is to split your original data set into a **train** portion (for developing your machine learning model) and a **test** portion (for evaluating the performance of the trained model).\n",
    "\n",
    "The simplest and most straightforward way to partition your data set is to randomly split it in two groups (*hold-out strategy*).\n",
    "\n",
    "\n",
    "---\n",
    "\n",
    "\n",
    "\"But we already have a dataset split into train and test!\", you may object.\n",
    "\n",
    "Well, the data was previously split into balanced train and test sets of 136 samples each. This is somewhat different from the 80/20 train/test split we mentioned in the lecture. In fact, this specific data split was created ad-hoc during the article preparation to ensure balance in the various clinical characteristics of the neuroblastoma samples represented (e.g. MYCN amplification status, INSS tumor staging, â€¦ Full details are reported in the article).\n",
    "\n",
    "For the sake of this tutorial, we will further split the neuroblastoma train set into two subsets.\n",
    "\n",
    "\n",
    "---\n",
    "\n",
    "\n",
    "You achieve this using scikit-learn's function `train_test_split`, in the `model_selection` submodule.\n",
    "\n",
    "For example, let's split the data (x_tr) into 80% train and 20% test (note the argument `test_size=0.2`)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "tqbLHGVP2cAH"
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(x_tr, y_tr, test_size=0.2, random_state=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "TOKwQzhK8tVY"
   },
   "source": [
    "\n",
    "\n",
    "---\n",
    "\n",
    "What is the random_state?\n",
    "\n",
    "Whenever randomness is involved in a computer program, we need to rely on some sort of workaround because computers follow their instructions blindly and they are therefore completely predictable.\n",
    "\n",
    "One approach relies on *Pseudo-Random Number Generators* (PRNGs). \n",
    "\n",
    "PNRGs are algorithms that use mathematical formulas or precalculated tables to produce sequences of numbers that appear random. \n",
    "\n",
    "PNRGs are initialized by a *seed* (an integer), so that *the same seed yields the same sequence of pseudo-random numbers*. This is useful for reproduciblity.\n",
    "\n",
    "\n",
    "---\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "le9GlyN3xo9x"
   },
   "source": [
    "*Now, retrain a kNN model on X_train and evaluate its performance on X_test. Try using different random states for data splitting.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "n12boA3k3Neo"
   },
   "outputs": [],
   "source": [
    "from sklearn import metrics\n",
    "knn = neighbors.KNeighborsClassifier(n_neighbors=10)\n",
    "knn.fit(X_train, y_train)\n",
    "y_pred_knn = knn.predict(X_test)\n",
    "\n",
    "acc = metrics.accuracy_score(y_test, y_pred_knn)\n",
    "mcc = metrics.matthews_corrcoef(y_test, y_pred_knn)\n",
    "\n",
    "print(\"Accuracy = \" , acc)\n",
    "print(\"MCC = \", mcc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Homework\n",
    "\n",
    "* Use the other labels (\"SEX\", \"UK\") to train a classifier\n",
    "* Fit on the \"tr\" set and predict on \"ts\"\n",
    "* Use the hold-out strategy\n",
    "* Evaluate the performance for each type of label"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## What next?\n",
    "\n",
    "* Cross-validation\n",
    "* Other classifiers: Support vector machines, Random Forests, Neural nets\n",
    "* Feature ranking\n",
    "* Parameter tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "practical_neuroblastoma_partI_v0.2.ipynb",
   "provenance": [],
   "version": "0.3.2"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
